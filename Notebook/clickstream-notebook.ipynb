{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "764ad4a7-1095-44ca-b76c-f48323de42e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.functions import from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc558a1b-9ab7-429d-b9fb-29ae9df9dcd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Connecting Kafka Topic To Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19f6d945-c51c-4599-ae79-7de3ef6f5439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Real Time Data Streaming for `clickstream` topic from Confluent Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b1dc7ea-de24-4b9d-859d-ed92762be5e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Kafka parameters\n",
    "kafka_bootstrap_servers = \"\"  # Replace with your Kafka bootstrap server\n",
    "kafka_topic = \"clickstream\"\n",
    "\n",
    "# Confluent Cloud credentials (replace with your actual API Key and Secret)\n",
    "api_key = \"\"\n",
    "api_secret = \"\"\n",
    "\n",
    "# Set Kafka configuration to use the API Key and Secret for SASL authentication\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": kafka_bootstrap_servers,\n",
    "    \"subscribe\": kafka_topic,\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",  # Use SASL_SSL for encrypted communication\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",  # Authentication mechanism\n",
    "    \"kafka.sasl.jaas.config\": f\"org.apache.kafka.common.security.plain.PlainLoginModule required username='{api_key}' password='{api_secret}';\",\n",
    "    \"startingOffsets\": \"latest\"\n",
    "}\n",
    "\n",
    "# Read data from Kafka topic in real-time\n",
    "kafka_stream_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**kafka_options) \\\n",
    "    .load()\n",
    "\n",
    "# Select and display the streaming records\n",
    "streaming_records_df = kafka_stream_df.selectExpr(\"CAST(value AS STRING) as message\")\n",
    "\n",
    "# Define the schema for the incoming JSON message\n",
    "schema = \"\"\"\n",
    "    event STRING,\n",
    "    user_id STRING,\n",
    "    timestamp STRING,\n",
    "    page STRING,\n",
    "    referrer STRING,\n",
    "    device STRING\n",
    "\"\"\"\n",
    "\n",
    "# Parse the JSON value to create structured data\n",
    "parsed_df = streaming_records_df.select(from_json(\"message\", schema).alias(\"data\"))\n",
    "\n",
    "# Extract the individual columns from the 'data' struct\n",
    "parsed_df = parsed_df.select(\"data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e686d83e-6ef3-4fcc-9f78-114e40be9e5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test real-time data analysis on `clickstream` topic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75e4b94f-67c5-4e71-a12f-f8d3dc33d46f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the parsed data (for debugging)\n",
    "display(parsed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac67795f-1d97-41df-b765-e7b382cf3a78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Mounting ADLS Gen 2 To Azure Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "130972d5-e752-44f1-88b9-86385439ba99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/mydatalake has been unmounted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%python\n",
    "# Define Azure Data Lake details\n",
    "container_name = \"\"  # Replace with your Data Lake container name\n",
    "storage_account_name = \"\"  # Replace with your Storage Account name\n",
    "storage_account_key = \"\"  # Replace with your Storage Account Key\n",
    "\n",
    "# Unmount the directory if it is already mounted\n",
    "mount_point = \"/mnt/mydatalake\"\n",
    "if any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "    dbutils.fs.unmount(mount_point)\n",
    "\n",
    "# Mount the Azure Data Lake Store Gen2\n",
    "dbutils.fs.mount(\n",
    "    source=f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\",\n",
    "    mount_point=mount_point,  # Path in Databricks where your storage will be accessible\n",
    "    extra_configs={\n",
    "        f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": storage_account_key\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c24d87-c4d1-49a2-905f-6481e631701d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Printing data from mount storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9508494e-79dd-486f-bff6-586a46092bdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Assuming the data is in JSON format\n",
    "df = spark.read.json(\"/mnt/mydatalake/topics/\")\n",
    "\n",
    "# Show the first few records\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4dc34b5-edd0-4212-958a-3435d0994f55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Total Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb0053a-0a09-45e6-96f7-46a7edd63936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total records with null values: 453'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f\"Total records with null values: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef55fb1a-8aaf-492b-be7e-3c75005fcd2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Removing NULL Values and Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b2cd4a7-d47e-4ea5-a19d-cd5210b28195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Checking Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ccaf45f-39d8-4fe4-a56c-95820e80a36b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count null values for each column\n",
    "null_counts = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "\n",
    "# Display the null counts\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba6881f9-f45a-45da-856b-df976e28b63d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Removing Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b73b4cc2-ec51-456c-a80b-6310722a6b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Remove rows with null values in any column\n",
    "df_null_cleaned = df.dropna()\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "display(df_null_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98607b2a-1f2a-4182-ad77-8a3620804d7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Checking Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66a43705-50ce-4881-81ae-dfd6d18c727b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Group by all columns and count the occurrences\n",
    "duplicate_counts_df = df_null_cleaned.groupBy(parsed_df.columns).count()\n",
    "\n",
    "# Filter rows where count is greater than 1 (indicating duplicates)\n",
    "duplicates_df = duplicate_counts_df.filter(col(\"count\") > 1)\n",
    "\n",
    "# Display the duplicates\n",
    "display(duplicates_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad39f62c-e971-46ac-9fe4-5acc16ab1d64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fbdc53f-b0d7-4a24-8125-d6d2d172d2a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "df_ready = df_null_cleaned.dropDuplicates()\n",
    "\n",
    "display(df_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c74a808a-eb99-4c54-ba17-8a410e93c6bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec31ac2-b66f-4e9a-9f1f-ced4ee844a2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Filtering data for a specific device (e.g., Tablet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41809993-a488-4719-b061-f0f3017d3871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter data for tablet device\n",
    "device_tablet_df = df_ready.filter(df_ready.device == \"tablet\")\n",
    "\n",
    "# Show the first few records\n",
    "display(device_tablet_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4d74528-ca9f-4fcf-8237-265942c2c559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Grouping columns and checking count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfa1b5c6-a36e-4d71-91ba-2578317255a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform groupby on the 'device' column and count the occurrences\n",
    "\n",
    "display(df_ready.groupBy(\"device\").agg(count(\"device\").alias(\"count\")).orderBy(col(\"count\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d079318-a92d-436c-aaee-d7837537b205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform groupby on the 'event' column and count the occurrences\n",
    "\n",
    "display(df_ready.groupBy(\"event\").agg(count(\"event\").alias(\"count\")).orderBy(col(\"count\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3bd2c25-866e-4301-ad79-cb35828d7480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform groupby on the 'page' column and count the occurrences\n",
    "\n",
    "display(df_ready.groupBy(\"page\").agg(count(\"page\").alias(\"count\")).orderBy(col(\"count\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9d0bc6c-7808-4deb-a54d-aead33d1c734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform groupby on the 'referrer' column and count the occurrences\n",
    "\n",
    "display(df_ready.groupBy(\"referrer\").agg(count(\"referrer\").alias(\"count\")).orderBy(col(\"count\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45dadcde-20f4-4651-8fb5-ac1ef26aff34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform groupby on the 'year', 'month' and 'day' columns and count the occurrences\n",
    "\n",
    "display(df_ready.groupBy(\"year\").agg(count(\"year\").alias(\"count\")).orderBy(col(\"count\").desc()))\n",
    "display(df_ready.groupBy(\"month\").agg(count(\"month\").alias(\"count\")).orderBy(col(\"count\").desc()))\n",
    "display(df_ready.groupBy(\"day\").agg(count(\"day\").alias(\"count\")).orderBy(col(\"count\").desc()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ad81add-fcbf-443d-867b-5eb1a3d1cf6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Saving our analysis into new directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33d61198-b24b-4baf-92fd-00c2a8af7edf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device tablet analysis saved!\n"
     ]
    }
   ],
   "source": [
    "# Path to save the data\n",
    "device_tablet_df_analysis_path = \"/mnt/mydatalake/analysis/device_tablet/\"\n",
    "\n",
    "# Save as csv with header\n",
    "device_tablet_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(device_tablet_df_analysis_path)\n",
    "\n",
    "print(\"Device tablet analysis saved!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "clickstream-notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
